import argparse
import os
import random
import warnings
import numpy as np
import torch
from transformers import AutoTokenizer
from transformers.generation.streamers import TextStreamer
from model.model import VibyConfig, VibyForCausalLM

warnings.filterwarnings("ignore")


def transfer_cache_to_device(cache, target_device):
    """Transfer cache state from one device to another."""
    if cache is None:
        return None

    # Create a new cache on target device
    from transformers.cache_utils import DynamicCache

    new_cache = DynamicCache()

    # Handle new cache format (v4.56+)
    if hasattr(cache, "layers") and cache.layers:
        # New format: cache.layers[layer_idx].keys, cache.layers[layer_idx].values
        for layer_idx, layer in enumerate(cache.layers):
            if (
                layer is not None
                and hasattr(layer, "keys")
                and hasattr(layer, "values")
            ):
                keys = layer.keys.to(target_device) if layer.keys is not None else None
                values = (
                    layer.values.to(target_device) if layer.values is not None else None
                )
                new_cache.update(keys, values, layer_idx)
            else:
                new_cache.update(None, None, layer_idx)

    # Transfer Canon cache states
    if hasattr(cache, "__dict__"):
        for attr_name, attr_value in cache.__dict__.items():
            if attr_name.startswith("canon_cache_") and isinstance(attr_value, list):
                new_attr_list = []
                for state in attr_value:
                    if state is not None:
                        new_attr_list.append(state.to(target_device))
                    else:
                        new_attr_list.append(None)
                setattr(new_cache, attr_name, new_attr_list)
            elif attr_name not in ["layers"]:
                # Transfer other attributes (like _max_layers, etc.)
                setattr(new_cache, attr_name, attr_value)

    return new_cache


def init_model(args):
    # Validate model_mode
    modes = {0: "pretrain", 1: "full_sft"}
    if args.model_mode not in modes:
        print(f"é”™è¯¯ï¼šä¸æ”¯æŒçš„æ¨¡å‹æ¨¡å¼ {args.model_mode}ï¼Œæ”¯æŒçš„æ¨¡å¼ï¼š{list(modes.keys())}")
        exit(1)
    
    # Check if tokenizer directory exists
    tokenizer_path = "./model/"
    if not os.path.exists(tokenizer_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°tokenizerç›®å½•: {tokenizer_path}")
        exit(1)
        
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    ckp = f"./{args.out_dir}/{modes[args.model_mode]}_{args.hidden_size}.pth"

    # Configure YaRN if enabled
    rope_scaling = None
    original_max_seq_len = args.max_seq_len  # ä¿å­˜åŸå§‹å€¼

    if hasattr(args, "enable_yarn") and args.enable_yarn:
        # è‡ªåŠ¨å°† max_seq_len ä¹˜ä»¥ scaling factor
        args.max_seq_len = int(args.max_seq_len * args.yarn_scaling_factor)

        rope_scaling = {
            "type": "yarn",
            "factor": args.yarn_scaling_factor,
            "beta_fast": getattr(args, "yarn_beta_fast", 32.0),
            "beta_slow": getattr(args, "yarn_beta_slow", 1.0),
        }
        print(
            f"[YaRN] å¯ç”¨ä¸Šä¸‹æ–‡æ‰©å±•: {original_max_seq_len} â†’ {args.max_seq_len} (scaling factor: {args.yarn_scaling_factor})"
        )

    config = VibyConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        max_position_embeddings=args.max_seq_len,
        original_max_position_embeddings=(
            original_max_seq_len
            if hasattr(args, "enable_yarn") and args.enable_yarn
            else getattr(args, "original_max_seq_len", 1024)
        ),
        rope_scaling=rope_scaling,
    )

    # Check if checkpoint file exists
    if not os.path.exists(ckp):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶: {ckp}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…ç¡®ä¿æ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆã€‚")
        exit(1)
    
    # Load checkpoint first
    checkpoint = torch.load(ckp, map_location="cpu")

    # Handle different checkpoint formats
    if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
        # New checkpoint format from training script
        state_dict = checkpoint["model_state_dict"]
    else:
        # Old format - direct state dict
        state_dict = checkpoint

    # Handle torch.compile prefix if present
    if any(key.startswith("_orig_mod.") for key in state_dict.keys()):
        state_dict = {
            key.replace("_orig_mod.", ""): value for key, value in state_dict.items()
        }

    # Check if hybrid mode is enabled
    if hasattr(args, "hybrid_mps_cpu") and args.hybrid_mps_cpu:
        # Create two models for hybrid inference
        prefill_device = getattr(args, "prefill_device", "mps")
        generation_device = getattr(args, "generation_device", "cpu")

        # Create prefill model (MPS)
        prefill_model = VibyForCausalLM(config)
        prefill_model.load_state_dict(state_dict, strict=True)
        prefill_model = prefill_model.eval().to(prefill_device)

        # Create generation model (CPU)
        generation_model = VibyForCausalLM(config)
        generation_model.load_state_dict(state_dict, strict=True)
        generation_model = generation_model.eval().to(generation_device)

        model_for_counting = prefill_model
        
        return_value = {
            "prefill_model": prefill_model,
            "generation_model": generation_model,
            "prefill_device": prefill_device,
            "generation_device": generation_device,
            "hybrid_mode": True,
        }
    else:
        # Original single-device mode
        model = VibyForCausalLM(config)
        model.load_state_dict(state_dict, strict=True)
        model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
        model = model.eval().to(args.device)
        
        model_for_counting = model
        return_value = model

    # è®¡ç®—å‚æ•°é‡ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
    total_params = sum(p.numel() for p in model_for_counting.parameters())
    trainable_params = sum(
        p.numel() for p in model_for_counting.parameters() if p.requires_grad
    )

    print(
        f"æ€»å‚æ•°é‡ï¼š{total_params / 1e6:.3f}M, å¯è®­ç»ƒå‚æ•°é‡ï¼š{trainable_params / 1e6:.3f}M"
    )

    return return_value, tokenizer


def get_prompt_datas(args):
    if args.model_mode == 0:
        # pretrainæ¨¡å‹çš„æ¥é¾™èƒ½åŠ›ï¼ˆæ— æ³•å¯¹è¯ï¼‰
        prompt_datas = [
            "é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†",
            "äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½",
            "ä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯",
            "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯",
            "äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­",
            "åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰",
            "æ­å·å¸‚çš„ç¾é£Ÿæœ‰",
        ]
    else:
        # é€šç”¨å¯¹è¯é—®é¢˜
        prompt_datas = [
            "è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
            "ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ",
            "é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ",
            "æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ",
            "è¯¦ç»†çš„ä»‹ç»å…‰é€Ÿçš„ç‰©ç†æ¦‚å¿µã€‚",
            "æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚",
            "è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚",
            "å¦‚ä½•ç†è§£ChatGPTï¼Ÿ",
            "Introduce the history of the United States, please.",
        ]

    return prompt_datas


# è®¾ç½®å¯å¤ç°çš„éšæœºç§å­
def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    parser = argparse.ArgumentParser(description="Chat with Viby")
    parser.add_argument("--lora_name", default="None", type=str)
    parser.add_argument("--out_dir", default="out", type=str)
    parser.add_argument("--temperature", default=0.85, type=float)
    parser.add_argument("--top_p", default=0.85, type=float)
    parser.add_argument(
        "--repetition_penalty",
        default=1.2,
        type=float,
    )
    parser.add_argument(
        "--device", default="cuda" if torch.cuda.is_available() else "cpu", type=str
    )
    parser.add_argument("--hidden_size", default=640, type=int)
    parser.add_argument("--num_hidden_layers", default=18, type=int)
    parser.add_argument("--max_seq_len", default=1024, type=int)
    parser.add_argument("--history_cnt", default=0, type=int, help="ä¿ç•™çš„å¯¹è¯å†å²è½®æ¬¡æ•°é‡ï¼ˆ0ä»£è¡¨æ— å†å²ï¼‰")
    parser.add_argument(
        "--model_mode",
        default=1,
        type=int,
        help="0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹",
    )
    # YaRN parameters
    parser.add_argument(
        "--enable_yarn", action="store_true", help="Enable YaRN scaling"
    )
    parser.add_argument(
        "--yarn_scaling_factor", default=2.0, type=float, help="YaRN scaling factor"
    )
    parser.add_argument(
        "--original_max_seq_len",
        default=512,
        type=int,
        help="Original context length before scaling",
    )
    parser.add_argument(
        "--yarn_beta_fast", default=32.0, type=float, help="YaRN beta_fast parameter"
    )
    parser.add_argument(
        "--yarn_beta_slow", default=1.0, type=float, help="YaRN beta_slow parameter"
    )

    # Hybrid inference parameters
    parser.add_argument(
        "--hybrid_mps_cpu",
        action="store_true",
        help="Enable hybrid inference: MPS for prefill, CPU for generation",
    )
    parser.add_argument(
        "--prefill_device",
        default="mps",
        type=str,
        help="Device for prefill phase (default: mps)",
    )
    parser.add_argument(
        "--generation_device",
        default="cpu",
        type=str,
        help="Device for generation phase (default: cpu)",
    )

    args = parser.parse_args()

    # Basic input validation
    if args.temperature <= 0:
        print("é”™è¯¯ï¼štemperature å¿…é¡»å¤§äº 0")
        exit(1)
    if args.top_p <= 0 or args.top_p > 1:
        print("é”™è¯¯ï¼štop_p å¿…é¡»åœ¨ (0, 1] èŒƒå›´å†…")
        exit(1)
    if args.repetition_penalty <= 0:
        print("é”™è¯¯ï¼šrepetition_penalty å¿…é¡»å¤§äº 0")
        exit(1)
    if args.max_seq_len <= 0:
        print("é”™è¯¯ï¼šmax_seq_len å¿…é¡»å¤§äº 0")
        exit(1)
    if args.history_cnt < 0:
        print("é”™è¯¯ï¼šhistory_cnt ä¸èƒ½ä¸ºè´Ÿæ•°")
        exit(1)

    model, tokenizer = init_model(args)

    prompts = get_prompt_datas(args)
    test_mode = int(input("[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n"))
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    messages = []
    for prompt in (prompts if test_mode == 0 else iter(lambda: input("ğŸ‘¶: "), "")):
        setup_seed(random.randint(0, 2048))
        # setup_seed(2025)  # å¦‚éœ€å›ºå®šæ¯æ¬¡è¾“å‡ºåˆ™æ¢æˆã€å›ºå®šã€‘çš„éšæœºç§å­
        if test_mode == 0:
            print(f"ğŸ‘¶: {prompt}")

        # å…ˆæ·»åŠ å½“å‰çš„ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": prompt})
        
        # ç„¶åæ ¹æ® history_cnt å¯¹æ•´ä¸ªæ¶ˆæ¯å†å²è¿›è¡Œæˆªæ–­ï¼ˆæŒ‰å¯¹è¯è½®æ¬¡ï¼‰
        if args.history_cnt > 0:
            # ä¸€ä¸ªå®Œæ•´çš„å¯¹è¯è½®æ¬¡æ˜¯2æ¡æ¶ˆæ¯ (user, assistant)
            # æˆ‘ä»¬è¦ä¿ç•™ history_cnt è½®å¯¹è¯ï¼Œå†åŠ ä¸Šå½“å‰åˆšè¾“å…¥çš„ç”¨æˆ·æ¶ˆæ¯
            # æ€»å…±éœ€è¦ä¿ç•™çš„æ¶ˆæ¯æ•°æ˜¯ 2 * history_cnt + 1
            num_to_keep = 2 * args.history_cnt + 1
            messages = messages[-num_to_keep:]
        else:
            # å¦‚æœ history_cnt ä¸º 0ï¼Œåˆ™æ¸…ç©ºå†å²ï¼Œåªä¿ç•™å½“å‰è¾“å…¥
            messages = messages[-1:]

        new_prompt = (
            tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            if args.model_mode != 0
            else (tokenizer.bos_token + prompt)
        )

        slice_start = 0
        if isinstance(model, dict) and model.get("hybrid_mode", False):
            prefill_model = model["prefill_model"]
            generation_model = model["generation_model"]
            prefill_device = model["prefill_device"]
            generation_device = model["generation_device"]

            inputs = tokenizer(new_prompt, return_tensors="pt", truncation=True).to(
                prefill_device
            )
            prompt_len = inputs["input_ids"].shape[1]
            max_new = max(1, args.max_seq_len - prompt_len)

            print("ğŸ¤–ï¸: ", end="")

            with torch.no_grad():
                outputs = prefill_model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    use_cache=True,
                )
                past_key_values = outputs.past_key_values

            past_key_values = transfer_cache_to_device(
                past_key_values, generation_device
            )

            # åªå– prompt çš„æœ€åä¸€ä¸ª token ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„åˆå§‹è¾“å…¥
            last_token_id = inputs["input_ids"][:, -1:].to(generation_device)
            # ç›´æ¥ä¼ é€’åŸå§‹attention maskï¼Œè®©generate()å†…éƒ¨å¤„ç†æ‰©å±•
            attention_mask = inputs["attention_mask"].to(generation_device)

            generated_ids = generation_model.generate(
                input_ids=last_token_id,  # ä¼ å…¥æœ€åä¸€ä¸ª token
                max_new_tokens=max_new,
                num_return_sequences=1,
                do_sample=True,
                attention_mask=attention_mask,  # ä¼ å…¥æ›´æ–°åçš„ attention_mask
                past_key_values=past_key_values,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                streamer=streamer,
                top_p=args.top_p,
                temperature=args.temperature,
                use_cache=True,
                repetition_penalty=args.repetition_penalty,
            )

            # generate()çš„è¾“å…¥æ˜¯last_token_id(é•¿åº¦ä¸º1)ï¼Œæ‰€ä»¥ä»ç´¢å¼•1å¼€å§‹è§£ç 
            slice_start = 1
        else:
            # Original single-device mode
            inputs = tokenizer(new_prompt, return_tensors="pt", truncation=True).to(
                args.device
            )
            # generateçš„è¾“å…¥æ˜¯å®Œæ•´çš„promptï¼Œæ‰€ä»¥ä»prompté•¿åº¦ä¹‹åå¼€å§‹è§£ç 
            slice_start = inputs["input_ids"].shape[1]
            prompt_len = inputs["input_ids"].shape[1]
            max_new = max(1, args.max_seq_len - prompt_len)

            print("ğŸ¤–ï¸: ", end="")
            generated_ids = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_new,
                num_return_sequences=1,
                do_sample=True,
                attention_mask=inputs["attention_mask"],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                streamer=streamer,
                top_p=args.top_p,
                temperature=args.temperature,
                use_cache=True,
                repetition_penalty=args.repetition_penalty,
            )

        # ç»Ÿä¸€å¤„ç†å“åº”è§£ç ï¼ˆç§»å‡º if/else å—ä»¥é¿å…é‡å¤ï¼‰
        response = tokenizer.decode(
            generated_ids[0][slice_start:], skip_special_tokens=True
        )
        messages.append({"role": "assistant", "content": response})
        print("\n\n")


if __name__ == "__main__":
    main()
