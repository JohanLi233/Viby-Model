#!/usr/bin/env python3
"""
æµ‹è¯• causal_conv1d çš„ MPS å®ç°æ­£ç¡®æ€§
"""

import torch
import torch.nn.functional as F
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import my_mps_extension._C as mps_ext


def causal_conv1d_reference(x, weight, bias=None, silu_activation=False):
    """
    ä½¿ç”¨ PyTorch å®ç°çš„å‚è€ƒç‰ˆæœ¬ï¼ˆCPUï¼‰
    x: (batch, dim, seqlen)
    weight: (dim, width)
    bias: (dim,) or None
    """
    batch, dim, seqlen = x.shape
    width = weight.shape[1]

    # è½¬æ¢ä¸ºCPUè¿›è¡Œå‚è€ƒè®¡ç®—
    x_cpu = x.detach().cpu().float()
    weight_cpu = weight.detach().cpu().float()
    bias_cpu = bias.detach().cpu().float() if bias is not None else None

    # ä½¿ç”¨ F.conv1d å®ç°å› æœå·ç§¯
    # æ·»åŠ  paddingï¼Œç„¶åæˆªå–æ­£ç¡®çš„éƒ¨åˆ†
    x_padded = F.pad(x_cpu, (width - 1, 0))  # åœ¨å·¦ä¾§å¡«å…… width-1 ä¸ªé›¶

    # ä½¿ç”¨åˆ†ç»„å·ç§¯
    out = F.conv1d(
        x_padded, weight_cpu.unsqueeze(1), bias=bias_cpu, groups=dim, padding=0
    )

    # æˆªå–åˆ°åŸå§‹åºåˆ—é•¿åº¦
    out = out[:, :, :seqlen]

    # åº”ç”¨ SiLU æ¿€æ´»å‡½æ•°
    if silu_activation:
        out = F.silu(out)

    return out


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•åŸºæœ¬åŠŸèƒ½ ===")

    # æµ‹è¯•å‚æ•°
    batch_size = 2
    dim = 4
    seqlen = 8
    width = 4

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    torch.manual_seed(42)
    x = torch.randn(batch_size, dim, seqlen, device="mps", dtype=torch.float32)
    weight = torch.randn(dim, width, device="mps", dtype=torch.float32)
    bias = torch.randn(dim, device="mps", dtype=torch.float32)

    print(f"è¾“å…¥å½¢çŠ¶: x {x.shape}, weight {weight.shape}, bias {bias.shape}")

    try:
        # MPS å®ç°
        print("è¿è¡Œ MPS å®ç°...")
        result_mps = mps_ext.causal_conv1d_fwd(x, weight, bias, False)
        print(f"MPS ç»“æœå½¢çŠ¶: {result_mps.shape}")

        # å‚è€ƒå®ç°
        print("è¿è¡Œå‚è€ƒå®ç°...")
        result_ref = causal_conv1d_reference(x, weight, bias, False)
        result_ref_mps = result_ref.to("mps")
        print(f"å‚è€ƒç»“æœå½¢çŠ¶: {result_ref_mps.shape}")

        # æ¯”è¾ƒç»“æœ
        diff = torch.abs(result_mps - result_ref_mps)
        max_diff = torch.max(diff).item()
        mean_diff = torch.mean(diff).item()

        print(f"æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print(f"å¹³å‡å·®å¼‚: {mean_diff:.6f}")

        # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        tolerance = 1e-4
        if max_diff < tolerance:
            print("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œå·®å¼‚è¿‡å¤§")
            print("MPS ç»“æœï¼ˆå‰å‡ ä¸ªå…ƒç´ ï¼‰:")
            print(result_mps[0, 0, :5])
            print("å‚è€ƒç»“æœï¼ˆå‰å‡ ä¸ªå…ƒç´ ï¼‰:")
            print(result_ref_mps[0, 0, :5])
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
        return False


def test_silu_activation():
    """æµ‹è¯• SiLU æ¿€æ´»å‡½æ•°"""
    print("\n=== æµ‹è¯• SiLU æ¿€æ´»å‡½æ•° ===")

    # ç®€å•æµ‹è¯•æ•°æ®
    batch_size = 1
    dim = 2
    seqlen = 4
    width = 4

    torch.manual_seed(123)
    x = torch.randn(batch_size, dim, seqlen, device="mps", dtype=torch.float32)
    weight = torch.randn(dim, width, device="mps", dtype=torch.float32)
    bias = torch.randn(dim, device="mps", dtype=torch.float32)

    try:
        # æµ‹è¯•ä¸å¸¦æ¿€æ´»
        result_no_act = mps_ext.causal_conv1d_fwd(x, weight, bias, False)
        ref_no_act = causal_conv1d_reference(x, weight, bias, False).to("mps")

        # æµ‹è¯•å¸¦æ¿€æ´»
        result_with_act = mps_ext.causal_conv1d_fwd(x, weight, bias, True)
        ref_with_act = causal_conv1d_reference(x, weight, bias, True).to("mps")

        # æ¯”è¾ƒ
        diff_no_act = torch.max(torch.abs(result_no_act - ref_no_act)).item()
        diff_with_act = torch.max(torch.abs(result_with_act - ref_with_act)).item()

        print(f"æ— æ¿€æ´»æœ€å¤§å·®å¼‚: {diff_no_act:.6f}")
        print(f"æœ‰æ¿€æ´»æœ€å¤§å·®å¼‚: {diff_with_act:.6f}")

        tolerance = 1e-4
        if diff_no_act < tolerance and diff_with_act < tolerance:
            print("âœ… SiLU æ¿€æ´»æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ SiLU æ¿€æ´»æµ‹è¯•å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ SiLU æ¿€æ´»æµ‹è¯•å‡ºé”™: {e}")
        return False


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\n=== æµ‹è¯•è¾¹ç•Œæƒ…å†µ ===")

    results = []

    # æµ‹è¯•æœ€å°å°ºå¯¸
    try:
        print("æµ‹è¯•æœ€å°å°ºå¯¸...")
        x = torch.randn(1, 1, 1, device="mps", dtype=torch.float32)
        weight = torch.randn(1, 4, device="mps", dtype=torch.float32)
        bias = torch.randn(1, device="mps", dtype=torch.float32)

        result = mps_ext.causal_conv1d_fwd(x, weight, bias, False)
        print(f"æœ€å°å°ºå¯¸ç»“æœ: {result.shape}")
        results.append(True)
    except Exception as e:
        print(f"âŒ æœ€å°å°ºå¯¸æµ‹è¯•å¤±è´¥: {e}")
        results.append(False)

    # æµ‹è¯•æ— åç½®
    try:
        print("æµ‹è¯•æ— åç½®...")
        x = torch.randn(2, 3, 5, device="mps", dtype=torch.float32)
        weight = torch.randn(3, 4, device="mps", dtype=torch.float32)
        bias = torch.tensor([], device="mps", dtype=torch.float32)  # ç©ºå¼ é‡

        # MPS å®ç°åº”è¯¥èƒ½å¤„ç†ç©ºçš„ bias
        result = mps_ext.causal_conv1d_fwd(x, weight, bias, False)
        print(f"æ— åç½®ç»“æœ: {result.shape}")
        results.append(True)
    except Exception as e:
        print(f"âŒ æ— åç½®æµ‹è¯•å¤±è´¥: {e}")
        results.append(False)

    all_passed = all(results)
    if all_passed:
        print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")
    else:
        print("âŒ éƒ¨åˆ†è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥")

    return all_passed


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• Causal Conv1D MPS å®ç°...")

    if not torch.backends.mps.is_available():
        print("âŒ MPS ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return

    results = []

    # è¿è¡Œå„é¡¹æµ‹è¯•
    results.append(test_basic_functionality())
    results.append(test_silu_activation())
    results.append(test_edge_cases())

    # æ€»ç»“
    print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
    passed = sum(results)
    total = len(results)
    print(f"é€šè¿‡: {passed}/{total}")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")


if __name__ == "__main__":
    main()
